---
title: "Energy Demand - testing_notebook"
author: "M Daniel A Turse"
date: "2021-10-04"
output:
  html_document:
    toc: yes
    toc_float:
      toc_collapsed: yes
    toc_depth: 3
    number_sections: yes
    theme: lumen
  word_document:
    toc: yes
    toc_depth: '3'
---


# Setup

## Allow for Reproducible Library Setup
```{r renv, message=TRUE, warning=FALSE}

renv::snapshot(
  prompt = FALSE
)

```


## Load the relevant libraries.
```{r libraries, message=FALSE, warning=FALSE}

# general data manipulation
library(tidyverse)     # data manipulation in general
library(lubridate)     # time-based data manipulation


# viz
library(GGally)
library(plotly)
library(tidytext)


# modeling
library(tidymodels)     # building models
library(modeltime)
library(modeltime.resample)
library(modeltime.ensemble)
library(mlflow)         # model tracking
library(DALEXtra)       # model analyses
library(vip)            # model analyses
# library(themis)         # smote sampling


# other
library(corrr)          # correlation analyses
library(janitor)        # data formatting
library(skimr)          # data inspection
library(tictoc)         # timing code
library(doParallel)     # parallelization across cores

```

## Session/Infrastructure Info
```{r session_info}

sessionInfo()

```


```{r installed_packages}

installed_packages <-
  tibble::as_tibble(installed.packages()) %>% 
  dplyr::select(
    Package,
    Version,
    LibPath
  ) %>% 
  dplyr::arrange(
    LibPath,
    Package
  )

installed_packages


rm(installed_packages)

```

## Options to Make Visuals Prettier
```{r setting_options}

# set the plotting theme to `theme_minimal`
ggplot2::theme_set(ggplot2::theme_minimal())
# set print options
options(max.print = 100000)   # set the number of lines to print
options(scipen = 999)         # digits longer than this will be converted to scientific notation

```

## Other Setup Optinos

Get the location of the working directory.
```{r set_wd}

wd <- here::here()
wd

```

Setup to allow for parallel processing across all cores.
```{r setup_parallel}

# reserving one core if using my laptop, zero if using AWS
# cores_to_save <-
#   if (wd == "/Users/mdturse/Dropbox/_AWS/") {
#     1
#   } else {
#     0
#   }
# 
# cores_to_use <- parallel::detectCores(logical = FALSE) - cores_to_save
# cores_to_use
# 
# cl <- parallel::makePSOCKcluster(cores_to_use)
# doParallel::registerDoParallel(cl)

```


# Data Manipulation

## Get the Raw Data

### Build Iterations of URL
```{r get_regions}

regions <-
  tibble::tibble(
    name = c("california", "carolinas", "central", "florida", "mid_atlantic", "midwest",
             "new_england", "new_york", "northwest", "southeast", "southwest", "tennessee",
             "texas"
             ),
    code = c("EBA.CAL-ALL.D.HL", "EBA.CAR-ALL.D.HL", "EBA.CENT-ALL.D.HL", "EBA.FLA-ALL.D.HL",
             "EBA.MIDA-ALL.D.HL", "EBA.MIDW-ALL.D.HL", "EBA.NE-ALL.D.HL", "EBA.NY-ALL.D.HL", 
             "EBA.NW-ALL.D.HL", "EBA.SE-ALL.D.HL", "EBA.SW-ALL.D.HL", "EBA.TEN-ALL.D.HL",
             "EBA.TEX-ALL.D.HL"
             )
  ) %>% 
  dplyr::mutate(
    series_id = paste0("&series_id=", code)
  )

regions

```


### Download Data

```{r get_json}

data.json <-
  purrr::pmap(
    .l = list(a = regions %>% dplyr::pull(series_id)),
    .f = function(a) {
      url_pre <- "http://api.eia.gov/series/?api_key="
      url_full <- paste0(url_pre, Sys.getenv("eia_api_key"), a)
      
      data_json <- jsonlite::fromJSON(txt = url_full)
      
      return(data_json)
    }
  )

names(data.json) <- regions$name

# data.json

```


```{r build_data_meta}

data.meta <-
  purrr::pmap_dfr(
    .l = list(a = data.json),
    .id = "region",
    .f = function(a) {
      
      data_meta <- a$series %>%
        dplyr::select(
          -data
        )
      
      return(data_meta)
    }
  )

data.meta

```


```{r build_data_raw}

data.raw <-
  purrr::pmap_dfr(
    .l = list(a = data.json),
    .id = "region",
    .f = function(a) {
      
      data_raw <-
        a$series %>%
        dplyr::pull(data) %>%
        tibble::as_tibble(.name_repair = ~ c("temp_col")) %>%
        dplyr::pull(temp_col) %>%
        tibble::as_tibble(.name_repair = ~ c("datetime", "demand_MWHrs")) %>%
        dplyr::mutate(
          datetime = lubridate::as_datetime(datetime, format = "%Y%m%dT%H-%M"),
          demand_MWHrs = as.integer(demand_MWHrs)
        )
      
      return(data_raw)
    }
  )

data.raw$region <- factor(data.raw$region)

glimpse(data.raw)

```

### Fix Data Issues
```{r data_fix_issues}

data.fix_issues <-
  data.raw %>% 
  dplyr::group_by(
    region
  ) %>% 
  dplyr::arrange(
    datetime
  ) %>% 
  dplyr::mutate(
    datetime = lubridate::floor_date(x = datetime, unit = "hours"),
    region_row_id = dplyr::row_number()
  ) %>% 
  timetk::pad_by_time(
    .date_var = datetime,
    .by = "hour",
    .pad_value = NA
  ) #%>% 
  # dplyr::ungroup()

glimpse(data.fix_issues)

# rm(data.raw)

```


```{r}

skimr::skim(data.fix_issues)
# data.fix_issues %>% dplyr::group_by(region) %>% skimr::skim()

```


```{r}

rm(data.json)

```


### Create Global Variables
```{r}

data.create_global_vars <-
  data.fix_issues %>% 
  timetk::tk_augment_fourier(
    .date_var = datetime,
    .periods = c(24, 24*7),
    .K = 2
  ) %>% 
  timetk::tk_augment_lags(
    .value = demand_MWHrs,
    .lags = c(1, 2, 24, 24*2, 24*7)
  ) %>% 
  timetk::tk_augment_slidify(
    .value = demand_MWHrs,
    .f = ~base::mean(.x, na.rm = TRUE),
    .period = c(1, 2, 24, 24*2, 24*7),
    .partial = FALSE,
    .align = "right"
  ) %>% 
  dplyr::filter(
    region_row_id >= ((24*7 + 1))
  )

glimpse(data.create_global_vars)

# rm(data.fix_issues)

```


# Parameters
```{r set_model_params}

# forecast four weeks ahead (24 hours x 7 days x 28 days)
forecast_horizon <- 24 * 7 * 28

```


# Nesting
```{r nest}

nested_data_tbl <-
  data.create_global_vars %>% 
  dplyr::group_by(
    region
  ) %>% 
  modeltime::extend_timeseries(
    .id_var = region,
    .date_var = datetime,
    .length_future = forecast_horizon
  ) %>% 
  modeltime::nest_timeseries(
    .id_var = region,
    .length_future = forecast_horizon
  ) %>% 
  modeltime::split_nested_timeseries(
    .length_test = forecast_horizon
  )

glimpse(nested_data_tbl)

```

# Modeling

## Recipes
```{r recipes}

rec.time <-
  recipes::recipe(
    demand_MWHrs ~ datetime,
    data = modeltime::extract_nested_train_split(
      .data = nested_data_tbl
    )
  )


rec.ml_base <-
  recipes::recipe(
    demand_MWHrs ~ .,
    data = modeltime::extract_nested_train_split(
      .data = nested_data_tbl
    )
  ) %>% 
  recipes::update_role(
    region_row_id,
    new_role = "id"
  ) %>% 
  recipes::step_impute_linear(
    demand_MWHrs,
    tidyselect::starts_with("demand_MWHrs_lag"),
    # impute_with = imp_vars(recipes::all_predictors())
    impute_with = imp_vars(demand_MWHrs_lag1, demand_MWHrs_lag2, demand_MWHrs_lag24,
                           demand_MWHrs_lag48, demand_MWHrs_lag168
                           )
  ) %>% 
  timetk::step_timeseries_signature(
    datetime
  ) %>% 
  # recipes::step_center(recipes::all_numeric_predictors()) %>% 
  # recipes::step_range(recipes::all_numeric_predictors()) %>% 
  recipes::step_rm(
    tidyselect::matches("(iso)|(xts)|(minute)|(second)|(hour12)"),
    datetime_month,
    datetime_day,
    datetime_qday,
    datetime_wday,
    datetime_yday
  ) %>% 
  recipes::step_mutate_at(
    datetime_year:datetime_mday7,
    fn = as.character
  ) %>% 
  recipes::step_center(recipes::all_numeric_predictors()) %>% 
  recipes::step_range(recipes::all_numeric_predictors()) %>% 
  recipes::step_nzv(recipes::all_numeric_predictors())

```


```{r}

rec.ml_base %>% 
  recipes::prep() %>% 
  recipes::bake(new_data = nested_data_tbl$.actual_data[[1]]) %>% 
  # dplyr::glimpse()
  skimr::skim()

```


## Models

### ARIMA

### Prophet

### GLMNet

### XGB


