---
title: "Energy Demand - testing_notebook"
author: "M Daniel A Turse"
date: "2021-10-04"
output:
  html_document:
    toc: yes
    toc_float:
      toc_collapsed: yes
    toc_depth: 3
    number_sections: yes
    theme: lumen
  word_document:
    toc: yes
    toc_depth: '3'
---


# Setup

## Allow for Reproducible Library Setup
```{r renv, message=TRUE, warning=FALSE}

renv::snapshot(
  prompt = FALSE
)

```


## Load the relevant libraries.
```{r libraries, message=FALSE, warning=FALSE}

# general data manipulation
library(tidyverse)     # data manipulation in general
library(lubridate)     # time-based data manipulation


# viz
library(GGally)
library(plotly)
library(tidytext)


# modeling
library(tidymodels)     # building models
library(modeltime)
library(modeltime.resample)
library(modeltime.ensemble)
library(mlflow)         # model tracking
library(DALEXtra)       # model analyses
library(vip)            # model analyses
# library(themis)         # smote sampling


# other
library(corrr)          # correlation analyses
library(janitor)        # data formatting
library(skimr)          # data inspection
library(tictoc)         # timing code
library(doParallel)     # parallelization across cores

```

## Session/Infrastructure Info
```{r session_info}

sessionInfo()

```


```{r installed_packages}

installed_packages <-
  tibble::as_tibble(installed.packages()) %>% 
  dplyr::select(
    Package,
    Version,
    LibPath
  ) %>% 
  dplyr::arrange(
    LibPath,
    Package
  )

installed_packages


rm(installed_packages)

```

## Options to Make Visuals Prettier
```{r setting_options}

# set the plotting theme to `theme_minimal`
ggplot2::theme_set(ggplot2::theme_minimal())
# set print options
options(max.print = 100000)   # set the number of lines to print
options(scipen = 999)         # digits longer than this will be converted to scientific notation

```

## Other Setup Optinos

Get the location of the working directory.
```{r set_wd}

wd <- here::here()
wd

```

Setup to allow for parallel processing across all cores.
```{r setup_parallel}

# reserving one core if using my laptop, zero if using AWS
# cores_to_save <-
#   if (wd == "/Users/mdturse/Dropbox/_AWS/") {
#     1
#   } else {
#     0
#   }
# 
# cores_to_use <- parallel::detectCores(logical = FALSE) - cores_to_save
# cores_to_use
# 
# cl <- parallel::makePSOCKcluster(cores_to_use)
# doParallel::registerDoParallel(cl)

```


# Data Manipulation

## Get the Raw Data

### Build Iterations of URL
```{r get_regions}

regions <-
  tibble::tibble(
    name = c("california", "carolinas", "central", "florida", "mid_atlantic", "midwest",
             "new_england", "new_york", "northwest", "southeast", "southwest", "tennessee",
             "texas"
             ),
    code = c("EBA.CAL-ALL.D.HL", "EBA.CAR-ALL.D.HL", "EBA.CENT-ALL.D.HL", "EBA.FLA-ALL.D.HL",
             "EBA.MIDA-ALL.D.HL", "EBA.MIDW-ALL.D.HL", "EBA.NE-ALL.D.HL", "EBA.NY-ALL.D.HL", 
             "EBA.NW-ALL.D.HL", "EBA.SE-ALL.D.HL", "EBA.SW-ALL.D.HL", "EBA.TEN-ALL.D.HL",
             "EBA.TEX-ALL.D.HL"
             )
  ) %>% 
  dplyr::mutate(
    series_id = paste0("&series_id=", code)
  )

regions

```


### Download Data

```{r get_json}

data.json <-
  purrr::pmap(
    .l = list(a = regions %>% dplyr::pull(series_id)),
    .f = function(a) {
      url_pre <- "http://api.eia.gov/series/?api_key="
      url_full <- paste0(url_pre, Sys.getenv("eia_api_key"), a)
      
      data_json <- jsonlite::fromJSON(txt = url_full)
      
      return(data_json)
    }
  )

names(data.json) <- regions$name

# data.json

```


```{r build_data_meta}

data.meta <-
  purrr::pmap_dfr(
    .l = list(a = data.json),
    .id = "region",
    .f = function(a) {
      
      data_meta <- a$series %>%
        dplyr::select(
          -data
        )
      
      return(data_meta)
    }
  )

data.meta

```


```{r build_data_raw}

data.raw <-
  purrr::pmap_dfr(
    .l = list(a = data.json),
    .id = "region",
    .f = function(a) {
      
      data_raw <-
        a$series %>%
        dplyr::pull(data) %>%
        tibble::as_tibble(.name_repair = ~ c("temp_col")) %>%
        dplyr::pull(temp_col) %>%
        tibble::as_tibble(.name_repair = ~ c("datetime", "demand_MWHrs")) %>%
        dplyr::mutate(
          datetime = lubridate::as_datetime(datetime, format = "%Y%m%dT%H-%M"),
          demand_MWHrs = as.integer(demand_MWHrs)
        )
      
      return(data_raw)
    }
  )

data.raw$region <- factor(data.raw$region)

glimpse(data.raw)

```


```{r data_fix_issues}

data.fix_issues <-
  data.raw %>% 
  dplyr::group_by(
    region
  ) %>% 
  dplyr::mutate(
    datetime = lubridate::floor_date(x = datetime, unit = "hours")
  ) %>% 
  timetk::pad_by_time(
    .date_var = datetime,
    .by = "hour",
    .pad_value = NA
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(
    region,
    datetime
  )

glimpse(data.fix_issues)

rm(data.raw)

```


```{r}

skimr::skim(data.fix_issues)
data.fix_issues %>% dplyr::group_by(region) %>% skimr::skim()

```


```{r}

rm(data.json)

```


# Parameters
```{r set_model_params}

# forecast four weeks ahead (24 hours x 7 days x 28 days)
forecast_horizon <- 24 * 7 * 28

```


# Nesting

```{r nest}

nested_data_tbl <-
  data.fix_issues %>% 
  dplyr::group_by(
    region
  ) %>% 
  modeltime::extend_timeseries(
    .id_var = region,
    .date_var = datetime,
    .length_future = forecast_horizon
  ) %>% 
  modeltime::nest_timeseries(
    .id_var = region,
    .length_future = forecast_horizon
  ) %>% 
  modeltime::split_nested_timeseries(
    .length_test = forecast_horizon
  )

glimpse(nested_data_tbl)

```

# Modeling

## Recipes
```{r recipes}

rec.time <-
  recipes::recipe(
    demand_MWHrs ~ datetime,
    data = modeltime::extract_nested_train_split(
      .data = nested_data_tbl
    )
  )

rec.ml_base <-
  recipes::recipe(
    demand_MWHrs ~ .,
    data = modeltime::extract_nested_train_split(
      .data = nested_data_tbl
    )
  )

```


## Models

### ARIMA

### Prophet

### GLMNet


